#ifndef CUFFTDX_FFT_4_FP64_FWD_PTX_HPP
#define CUFFTDX_FFT_4_FP64_FWD_PTX_HPP



template<> __forceinline__ __device__ void cufftdx_private_function<438, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .f64 fd<33>;\n\t"
    ".reg .b64 rd<2>;\n\t"
    "add.f64 fd17, %9, %14;\n\t"
    "add.f64 fd18, %10, %16;\n\t"
    "sub.f64 fd19, %9, %14;\n\t"
    "sub.f64 fd20, %10, %16;\n\t"
    "add.f64 fd21, %11, %17;\n\t"
    "add.f64 fd22, %13, %18;\n\t"
    "sub.f64 fd23, %11, %17;\n\t"
    "sub.f64 fd24, %13, %18;\n\t"
    "add.f64 %1, fd18, fd22;\n\t"
    "add.f64 %0, fd17, fd21;\n\t"
    "sub.f64 %3, fd20, fd23;\n\t"
    "add.f64 %2, fd19, fd24;\n\t"
    "sub.f64 %5, fd18, fd22;\n\t"
    "sub.f64 %4, fd17, fd21;\n\t"
    "add.f64 %7, fd20, fd23;\n\t"
    "sub.f64 %6, fd19, fd24;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y), "=d"(rmem[2].x), "=d"(rmem[2].y), "=d"(rmem[3].x), "=d"(rmem[3].y): "l"(smem), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y), "d"(rmem[1].y), "d"(rmem[2].x), "d"(rmem[2].y), "d"(rmem[2].y), "d"(rmem[3].x), "d"(rmem[3].y));
};




template<> __forceinline__ __device__ void cufftdx_private_function<439, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .b32 r<17>;\n\t"
    ".reg .f64 fd<30>;\n\t"
    ".reg .b64 rd<5>;\n\t"
    "mov.u32 r5, %tid.x;\n\t"
    "add.f64 fd1, %6, %8;\n\t"
    "add.f64 fd2, %7, %9;\n\t"
    "sub.f64 fd15, %6, %8;\n\t"
    "sub.f64 fd16, %7, %9;\n\t"
    "and.b32 r1, r5, 1;\n\t"
    "mul.wide.u32 rd2, r1, 16;\n\t"
    "mov.u64 rd3, %5;\n\t"
    "add.s64 rd4, rd3, rd2;\n\t"
    "ld.global.v2.f64 {fd17, fd18}, [rd4];\n\t"
    "mul.f64 fd21, fd17, fd15;\n\t"
    "mul.f64 fd22, fd18, fd16;\n\t"
    "sub.f64 fd3, fd21, fd22;\n\t"
    "mul.f64 fd23, fd17, fd16;\n\t"
    "fma.rn.f64 fd4, fd18, fd15, fd23;\n\t"
    "mov.u32 r6, %tid.y;\n\t"
    "shl.b32 r7, r6, 2;\n\t"
    "shl.b32 r8, r5, 1;\n\t"
    "and.b32 r9, r8, -4;\n\t"
    "add.s32 r2, r9, r7;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r10, r1, 1;\n\t"
    "add.s32 r11, r10, r2;\n\t"
    "shl.b32 r12, r11, 3;\n\t"
    "{\n\t"
    ".reg .u64 wide1;\n\t"
    ".reg .u32 narrow1;\n\t"
    "mov.u64 wide1, %4;\n\t"
    "cvt.u32.u64 narrow1, wide1;\n\t"
    "cvta.to.shared.u32 r13, narrow1;\n\t"
    "}\n\t"
    "add.s32 r3, r13, r12;\n\t"
    "st.shared.f64 [r3], fd1;\n\t"
    "st.shared.f64 [r3+8], fd3;\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r14, r1, r2;\n\t"
    "shl.b32 r15, r14, 3;\n\t"
    "add.s32 r4, r13, r15;\n\t"
    "ld.shared.f64 fd5, [r4];\n\t"
    "ld.shared.f64 fd6, [r4+16];\n\t"
    "barrier.sync 0;\n\t"
    "st.shared.f64 [r3], fd2;\n\t"
    "st.shared.f64 [r3+8], fd4;\n\t"
    "barrier.sync 0;\n\t"
    "ld.shared.f64 fd24, [r4];\n\t"
    "ld.shared.f64 fd25, [r4+16];\n\t"
    "add.f64 %0, fd5, fd6;\n\t"
    "add.f64 %1, fd24, fd25;\n\t"
    "sub.f64 %2, fd5, fd6;\n\t"
    "sub.f64 %3, fd24, fd25;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y): "l"(smem), "l"(lut_dp_2_4), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y));
};




template<> __forceinline__ __device__ void cufftdx_private_function<440, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .b32 r<17>;\n\t"
    ".reg .f64 fd<34>;\n\t"
    ".reg .b64 rd<5>;\n\t"
    "mov.u32 r3, %tid.x;\n\t"
    "add.f64 fd1, %6, %8;\n\t"
    "add.f64 fd2, %7, %9;\n\t"
    "sub.f64 fd13, %6, %8;\n\t"
    "sub.f64 fd14, %7, %9;\n\t"
    "and.b32 r1, r3, 1;\n\t"
    "mul.wide.u32 rd2, r1, 16;\n\t"
    "mov.u64 rd3, %5;\n\t"
    "add.s64 rd4, rd3, rd2;\n\t"
    "ld.global.v2.f64 {fd15, fd16}, [rd4];\n\t"
    "mul.f64 fd19, fd15, fd13;\n\t"
    "mul.f64 fd20, fd16, fd14;\n\t"
    "sub.f64 fd3, fd19, fd20;\n\t"
    "mul.f64 fd21, fd15, fd14;\n\t"
    "fma.rn.f64 fd4, fd16, fd13, fd21;\n\t"
    "mov.u32 r4, %tid.y;\n\t"
    "shl.b32 r5, r4, 2;\n\t"
    "shl.b32 r6, r3, 1;\n\t"
    "and.b32 r7, r6, -4;\n\t"
    "add.s32 r2, r7, r5;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r8, r1, 1;\n\t"
    "add.s32 r9, r8, r2;\n\t"
    "shl.b32 r10, r9, 4;\n\t"
    "{\n\t"
    ".reg .u64 wide1;\n\t"
    ".reg .u32 narrow1;\n\t"
    "mov.u64 wide1, %4;\n\t"
    "cvt.u32.u64 narrow1, wide1;\n\t"
    "cvta.to.shared.u32 r11, narrow1;\n\t"
    "}\n\t"
    "add.s32 r12, r11, r10;\n\t"
    "st.shared.v2.f64 [r12], {fd1, fd2};\n\t"
    "st.shared.v2.f64 [r12+16], {fd3, fd4};\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r13, r1, r2;\n\t"
    "shl.b32 r14, r13, 4;\n\t"
    "add.s32 r16, r11, r14;\n\t"
    "ld.shared.v2.f64 {fd22, fd23}, [r16];\n\t"
    "ld.shared.v2.f64 {fd26, fd27}, [r16+32];\n\t"
    "add.f64 %1, fd23, fd27;\n\t"
    "add.f64 %0, fd22, fd26;\n\t"
    "sub.f64 %3, fd23, fd27;\n\t"
    "sub.f64 %2, fd22, fd26;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y): "l"(smem), "l"(lut_dp_2_4), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y));
};


#endif
