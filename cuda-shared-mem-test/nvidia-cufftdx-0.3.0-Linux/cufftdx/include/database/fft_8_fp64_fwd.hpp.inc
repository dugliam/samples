#ifndef CUFFTDX_FFT_8_FP64_FWD_PTX_HPP
#define CUFFTDX_FFT_8_FP64_FWD_PTX_HPP



template<> __forceinline__ __device__ void cufftdx_private_function<441, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .f64 fd<90>;\n\t"
    ".reg .b64 rd<2>;\n\t"
    "add.f64 fd33, %17, %27;\n\t"
    "add.f64 fd34, %18, %29;\n\t"
    "sub.f64 fd35, %17, %27;\n\t"
    "sub.f64 fd36, %18, %29;\n\t"
    "add.f64 fd37, %22, %33;\n\t"
    "add.f64 fd38, %24, %34;\n\t"
    "sub.f64 fd39, %22, %33;\n\t"
    "sub.f64 fd40, %24, %34;\n\t"
    "add.f64 fd41, fd33, fd37;\n\t"
    "add.f64 fd42, fd34, fd38;\n\t"
    "sub.f64 fd43, fd33, fd37;\n\t"
    "sub.f64 fd44, fd34, fd38;\n\t"
    "add.f64 fd45, fd35, fd40;\n\t"
    "sub.f64 fd46, fd36, fd39;\n\t"
    "sub.f64 fd47, fd35, fd40;\n\t"
    "add.f64 fd48, fd36, fd39;\n\t"
    "add.f64 fd49, %19, %30;\n\t"
    "add.f64 fd50, %21, %32;\n\t"
    "sub.f64 fd51, %19, %30;\n\t"
    "sub.f64 fd52, %21, %32;\n\t"
    "add.f64 fd53, %25, %35;\n\t"
    "add.f64 fd54, %26, %36;\n\t"
    "sub.f64 fd55, %25, %35;\n\t"
    "sub.f64 fd56, %26, %36;\n\t"
    "add.f64 fd57, fd49, fd53;\n\t"
    "add.f64 fd58, fd50, fd54;\n\t"
    "sub.f64 fd59, fd49, fd53;\n\t"
    "sub.f64 fd60, fd50, fd54;\n\t"
    "add.f64 fd61, fd51, fd56;\n\t"
    "sub.f64 fd62, fd52, fd55;\n\t"
    "sub.f64 fd63, fd51, fd56;\n\t"
    "add.f64 fd64, fd52, fd55;\n\t"
    "mul.f64 fd65, fd61, 0d3FE6A09E667F3BCD;\n\t"
    "mul.f64 fd66, fd62, 0dBFE6A09E667F3BCD;\n\t"
    "sub.f64 fd67, fd65, fd66;\n\t"
    "mul.f64 fd68, fd62, 0d3FE6A09E667F3BCD;\n\t"
    "fma.rn.f64 fd69, fd61, 0dBFE6A09E667F3BCD, fd68;\n\t"
    "mul.f64 fd70, fd63, 0dBFE6A09E667F3BCD;\n\t"
    "mul.f64 fd71, fd64, 0dBFE6A09E667F3BCD;\n\t"
    "sub.f64 fd72, fd70, fd71;\n\t"
    "add.f64 fd73, fd70, fd71;\n\t"
    "add.f64 %1, fd42, fd58;\n\t"
    "add.f64 %0, fd41, fd57;\n\t"
    "add.f64 %3, fd46, fd69;\n\t"
    "add.f64 %2, fd45, fd67;\n\t"
    "sub.f64 %5, fd44, fd59;\n\t"
    "add.f64 %4, fd43, fd60;\n\t"
    "add.f64 %7, fd48, fd73;\n\t"
    "add.f64 %6, fd47, fd72;\n\t"
    "sub.f64 %9, fd42, fd58;\n\t"
    "sub.f64 %8, fd41, fd57;\n\t"
    "sub.f64 %11, fd46, fd69;\n\t"
    "sub.f64 %10, fd45, fd67;\n\t"
    "add.f64 %13, fd44, fd59;\n\t"
    "sub.f64 %12, fd43, fd60;\n\t"
    "sub.f64 %15, fd48, fd73;\n\t"
    "sub.f64 %14, fd47, fd72;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y), "=d"(rmem[2].x), "=d"(rmem[2].y), "=d"(rmem[3].x), "=d"(rmem[3].y), "=d"(rmem[4].x), "=d"(rmem[4].y), "=d"(rmem[5].x), "=d"(rmem[5].y), "=d"(rmem[6].x), "=d"(rmem[6].y), "=d"(rmem[7].x), "=d"(rmem[7].y): "l"(smem), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y), "d"(rmem[1].y), "d"(rmem[2].x), "d"(rmem[2].y), "d"(rmem[2].y), "d"(rmem[3].x), "d"(rmem[3].y), "d"(rmem[4].x), "d"(rmem[4].y), "d"(rmem[4].y), "d"(rmem[5].x), "d"(rmem[5].y), "d"(rmem[5].y), "d"(rmem[6].x), "d"(rmem[6].y), "d"(rmem[7].x), "d"(rmem[7].y));
};




template<> __forceinline__ __device__ void cufftdx_private_function<442, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .b32 r<17>;\n\t"
    ".reg .f64 fd<77>;\n\t"
    ".reg .b64 rd<5>;\n\t"
    "mov.u32 r5, %tid.x;\n\t"
    "add.f64 fd29, %10, %15;\n\t"
    "add.f64 fd30, %11, %17;\n\t"
    "sub.f64 fd31, %10, %15;\n\t"
    "sub.f64 fd32, %11, %17;\n\t"
    "add.f64 fd33, %12, %18;\n\t"
    "add.f64 fd34, %14, %19;\n\t"
    "sub.f64 fd35, %12, %18;\n\t"
    "sub.f64 fd36, %14, %19;\n\t"
    "add.f64 fd1, fd29, fd33;\n\t"
    "add.f64 fd2, fd30, fd34;\n\t"
    "sub.f64 fd37, fd29, fd33;\n\t"
    "sub.f64 fd38, fd30, fd34;\n\t"
    "add.f64 fd39, fd31, fd36;\n\t"
    "sub.f64 fd40, fd32, fd35;\n\t"
    "sub.f64 fd41, fd31, fd36;\n\t"
    "add.f64 fd42, fd32, fd35;\n\t"
    "and.b32 r1, r5, 1;\n\t"
    "mul.wide.u32 rd2, r1, 16;\n\t"
    "mov.u64 rd3, %9;\n\t"
    "add.s64 rd4, rd3, rd2;\n\t"
    "ld.global.v2.f64 {fd43, fd44}, [rd4];\n\t"
    "mul.f64 fd47, fd43, fd39;\n\t"
    "mul.f64 fd48, fd44, fd40;\n\t"
    "sub.f64 fd3, fd47, fd48;\n\t"
    "mul.f64 fd49, fd43, fd40;\n\t"
    "fma.rn.f64 fd4, fd44, fd39, fd49;\n\t"
    "mul.f64 fd50, fd43, fd43;\n\t"
    "mul.f64 fd51, fd44, fd44;\n\t"
    "sub.f64 fd52, fd50, fd51;\n\t"
    "mul.f64 fd53, fd44, fd43;\n\t"
    "fma.rn.f64 fd54, fd44, fd43, fd53;\n\t"
    "mul.f64 fd55, fd52, fd37;\n\t"
    "mul.f64 fd56, fd54, fd38;\n\t"
    "sub.f64 fd5, fd55, fd56;\n\t"
    "mul.f64 fd57, fd52, fd38;\n\t"
    "fma.rn.f64 fd6, fd54, fd37, fd57;\n\t"
    "ld.global.v2.f64 {fd58, fd59}, [rd4+32];\n\t"
    "mul.f64 fd62, fd58, fd41;\n\t"
    "mul.f64 fd63, fd59, fd42;\n\t"
    "sub.f64 fd7, fd62, fd63;\n\t"
    "mul.f64 fd64, fd58, fd42;\n\t"
    "fma.rn.f64 fd8, fd59, fd41, fd64;\n\t"
    "mov.u32 r6, %tid.y;\n\t"
    "shl.b32 r7, r6, 3;\n\t"
    "shl.b32 r8, r5, 2;\n\t"
    "and.b32 r9, r8, -8;\n\t"
    "add.s32 r2, r9, r7;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r10, r1, 2;\n\t"
    "add.s32 r11, r10, r2;\n\t"
    "shl.b32 r12, r11, 3;\n\t"
    "{\n\t"
    ".reg .u64 wide1;\n\t"
    ".reg .u32 narrow1;\n\t"
    "mov.u64 wide1, %8;\n\t"
    "cvt.u32.u64 narrow1, wide1;\n\t"
    "cvta.to.shared.u32 r13, narrow1;\n\t"
    "}\n\t"
    "add.s32 r3, r13, r12;\n\t"
    "st.shared.f64 [r3], fd1;\n\t"
    "st.shared.f64 [r3+8], fd3;\n\t"
    "st.shared.f64 [r3+16], fd5;\n\t"
    "st.shared.f64 [r3+24], fd7;\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r14, r1, r2;\n\t"
    "shl.b32 r15, r14, 3;\n\t"
    "add.s32 r4, r13, r15;\n\t"
    "ld.shared.f64 fd9, [r4];\n\t"
    "ld.shared.f64 fd10, [r4+16];\n\t"
    "ld.shared.f64 fd11, [r4+32];\n\t"
    "ld.shared.f64 fd12, [r4+48];\n\t"
    "barrier.sync 0;\n\t"
    "st.shared.f64 [r3], fd2;\n\t"
    "st.shared.f64 [r3+8], fd4;\n\t"
    "st.shared.f64 [r3+16], fd6;\n\t"
    "st.shared.f64 [r3+24], fd8;\n\t"
    "barrier.sync 0;\n\t"
    "ld.shared.f64 fd65, [r4];\n\t"
    "ld.shared.f64 fd66, [r4+16];\n\t"
    "ld.shared.f64 fd67, [r4+32];\n\t"
    "ld.shared.f64 fd68, [r4+48];\n\t"
    "add.f64 %0, fd9, fd11;\n\t"
    "add.f64 %1, fd65, fd67;\n\t"
    "add.f64 %2, fd10, fd12;\n\t"
    "add.f64 %3, fd66, fd68;\n\t"
    "sub.f64 %4, fd9, fd11;\n\t"
    "sub.f64 %5, fd65, fd67;\n\t"
    "sub.f64 %6, fd10, fd12;\n\t"
    "sub.f64 %7, fd66, fd68;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y), "=d"(rmem[2].x), "=d"(rmem[2].y), "=d"(rmem[3].x), "=d"(rmem[3].y): "l"(smem), "l"(lut_dp_4_8), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y), "d"(rmem[1].y), "d"(rmem[2].x), "d"(rmem[2].y), "d"(rmem[2].y), "d"(rmem[3].x), "d"(rmem[3].y));
};




template<> __forceinline__ __device__ void cufftdx_private_function<443, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .b32 r<30>;\n\t"
    ".reg .f64 fd<47>;\n\t"
    ".reg .b64 rd<8>;\n\t"
    "mov.u32 r1, %tid.x;\n\t"
    "add.f64 fd1, %7, %9;\n\t"
    "add.f64 fd2, %8, %10;\n\t"
    "sub.f64 fd21, %7, %9;\n\t"
    "sub.f64 fd22, %8, %10;\n\t"
    "and.b32 r2, r1, 3;\n\t"
    "mul.wide.u32 rd2, r2, 16;\n\t"
    "mov.u64 rd3, %5;\n\t"
    "add.s64 rd4, rd3, rd2;\n\t"
    "ld.global.v2.f64 {fd23, fd24}, [rd4];\n\t"
    "mul.f64 fd27, fd23, fd21;\n\t"
    "mul.f64 fd28, fd24, fd22;\n\t"
    "sub.f64 fd3, fd27, fd28;\n\t"
    "mul.f64 fd29, fd23, fd22;\n\t"
    "fma.rn.f64 fd4, fd24, fd21, fd29;\n\t"
    "mov.u32 r10, %tid.y;\n\t"
    "shl.b32 r11, r10, 3;\n\t"
    "shl.b32 r12, r1, 1;\n\t"
    "and.b32 r13, r12, -8;\n\t"
    "add.s32 r3, r13, r11;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r14, r2, 1;\n\t"
    "add.s32 r15, r14, r3;\n\t"
    "shl.b32 r16, r15, 3;\n\t"
    "{\n\t"
    ".reg .u64 wide1;\n\t"
    ".reg .u32 narrow1;\n\t"
    "mov.u64 wide1, %4;\n\t"
    "cvt.u32.u64 narrow1, wide1;\n\t"
    "cvta.to.shared.u32 r17, narrow1;\n\t"
    "}\n\t"
    "add.s32 r4, r17, r16;\n\t"
    "st.shared.f64 [r4], fd1;\n\t"
    "st.shared.f64 [r4+8], fd3;\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r18, r2, r3;\n\t"
    "shl.b32 r19, r18, 3;\n\t"
    "add.s32 r5, r17, r19;\n\t"
    "ld.shared.f64 fd5, [r5];\n\t"
    "ld.shared.f64 fd6, [r5+32];\n\t"
    "barrier.sync 0;\n\t"
    "st.shared.f64 [r4], fd2;\n\t"
    "st.shared.f64 [r4+8], fd4;\n\t"
    "barrier.sync 0;\n\t"
    "ld.shared.f64 fd30, [r5];\n\t"
    "ld.shared.f64 fd31, [r5+32];\n\t"
    "add.f64 fd7, fd5, fd6;\n\t"
    "add.f64 fd8, fd30, fd31;\n\t"
    "sub.f64 fd32, fd5, fd6;\n\t"
    "sub.f64 fd33, fd30, fd31;\n\t"
    "and.b32 r6, r1, 2;\n\t"
    "bfe.u32 r21, r1, 1, 1;\n\t"
    "mul.wide.u32 rd5, r21, 16;\n\t"
    "mov.u64 rd6, %6;\n\t"
    "add.s64 rd7, rd6, rd5;\n\t"
    "ld.global.v2.f64 {fd34, fd35}, [rd7];\n\t"
    "mul.f64 fd38, fd34, fd32;\n\t"
    "mul.f64 fd39, fd35, fd33;\n\t"
    "sub.f64 fd9, fd38, fd39;\n\t"
    "mul.f64 fd40, fd34, fd33;\n\t"
    "fma.rn.f64 fd10, fd35, fd32, fd40;\n\t"
    "and.b32 r22, r1, 1;\n\t"
    "add.s32 r7, r3, r22;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r23, r6, 1;\n\t"
    "add.s32 r24, r23, r7;\n\t"
    "shl.b32 r25, r24, 3;\n\t"
    "add.s32 r8, r17, r25;\n\t"
    "st.shared.f64 [r8], fd7;\n\t"
    "st.shared.f64 [r8+16], fd9;\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r27, r6, r7;\n\t"
    "shl.b32 r28, r27, 3;\n\t"
    "add.s32 r9, r17, r28;\n\t"
    "ld.shared.f64 fd11, [r9];\n\t"
    "ld.shared.f64 fd12, [r9+32];\n\t"
    "barrier.sync 0;\n\t"
    "st.shared.f64 [r8], fd8;\n\t"
    "st.shared.f64 [r8+16], fd10;\n\t"
    "barrier.sync 0;\n\t"
    "ld.shared.f64 fd41, [r9];\n\t"
    "ld.shared.f64 fd42, [r9+32];\n\t"
    "add.f64 %0, fd11, fd12;\n\t"
    "add.f64 %1, fd41, fd42;\n\t"
    "sub.f64 %2, fd11, fd12;\n\t"
    "sub.f64 %3, fd41, fd42;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y): "l"(smem), "l"(lut_dp_2_8), "l"(lut_dp_2_4), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y));
};




template<> __forceinline__ __device__ void cufftdx_private_function<444, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .b32 r<17>;\n\t"
    ".reg .f64 fd<85>;\n\t"
    ".reg .b64 rd<5>;\n\t"
    "mov.u32 r3, %tid.x;\n\t"
    "add.f64 fd25, %10, %15;\n\t"
    "add.f64 fd26, %11, %17;\n\t"
    "sub.f64 fd27, %10, %15;\n\t"
    "sub.f64 fd28, %11, %17;\n\t"
    "add.f64 fd29, %12, %18;\n\t"
    "add.f64 fd30, %14, %19;\n\t"
    "sub.f64 fd31, %12, %18;\n\t"
    "sub.f64 fd32, %14, %19;\n\t"
    "add.f64 fd1, fd25, fd29;\n\t"
    "add.f64 fd2, fd26, fd30;\n\t"
    "sub.f64 fd33, fd25, fd29;\n\t"
    "sub.f64 fd34, fd26, fd30;\n\t"
    "add.f64 fd35, fd27, fd32;\n\t"
    "sub.f64 fd36, fd28, fd31;\n\t"
    "sub.f64 fd37, fd27, fd32;\n\t"
    "add.f64 fd38, fd28, fd31;\n\t"
    "and.b32 r1, r3, 1;\n\t"
    "mul.wide.u32 rd2, r1, 16;\n\t"
    "mov.u64 rd3, %9;\n\t"
    "add.s64 rd4, rd3, rd2;\n\t"
    "ld.global.v2.f64 {fd39, fd40}, [rd4];\n\t"
    "mul.f64 fd43, fd39, fd35;\n\t"
    "mul.f64 fd44, fd40, fd36;\n\t"
    "sub.f64 fd3, fd43, fd44;\n\t"
    "mul.f64 fd45, fd39, fd36;\n\t"
    "fma.rn.f64 fd4, fd40, fd35, fd45;\n\t"
    "mul.f64 fd46, fd39, fd39;\n\t"
    "mul.f64 fd47, fd40, fd40;\n\t"
    "sub.f64 fd48, fd46, fd47;\n\t"
    "mul.f64 fd49, fd40, fd39;\n\t"
    "fma.rn.f64 fd50, fd40, fd39, fd49;\n\t"
    "mul.f64 fd51, fd48, fd33;\n\t"
    "mul.f64 fd52, fd50, fd34;\n\t"
    "sub.f64 fd5, fd51, fd52;\n\t"
    "mul.f64 fd53, fd48, fd34;\n\t"
    "fma.rn.f64 fd6, fd50, fd33, fd53;\n\t"
    "ld.global.v2.f64 {fd54, fd55}, [rd4+32];\n\t"
    "mul.f64 fd58, fd54, fd37;\n\t"
    "mul.f64 fd59, fd55, fd38;\n\t"
    "sub.f64 fd7, fd58, fd59;\n\t"
    "mul.f64 fd60, fd54, fd38;\n\t"
    "fma.rn.f64 fd8, fd55, fd37, fd60;\n\t"
    "mov.u32 r4, %tid.y;\n\t"
    "shl.b32 r5, r4, 3;\n\t"
    "shl.b32 r6, r3, 2;\n\t"
    "and.b32 r7, r6, -8;\n\t"
    "add.s32 r2, r7, r5;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r8, r1, 2;\n\t"
    "add.s32 r9, r8, r2;\n\t"
    "shl.b32 r10, r9, 4;\n\t"
    "{\n\t"
    ".reg .u64 wide1;\n\t"
    ".reg .u32 narrow1;\n\t"
    "mov.u64 wide1, %8;\n\t"
    "cvt.u32.u64 narrow1, wide1;\n\t"
    "cvta.to.shared.u32 r11, narrow1;\n\t"
    "}\n\t"
    "add.s32 r12, r11, r10;\n\t"
    "st.shared.v2.f64 [r12], {fd1, fd2};\n\t"
    "st.shared.v2.f64 [r12+16], {fd3, fd4};\n\t"
    "st.shared.v2.f64 [r12+32], {fd5, fd6};\n\t"
    "st.shared.v2.f64 [r12+48], {fd7, fd8};\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r13, r1, r2;\n\t"
    "shl.b32 r14, r13, 4;\n\t"
    "add.s32 r16, r11, r14;\n\t"
    "ld.shared.v2.f64 {fd61, fd62}, [r16];\n\t"
    "ld.shared.v2.f64 {fd65, fd66}, [r16+32];\n\t"
    "ld.shared.v2.f64 {fd69, fd70}, [r16+64];\n\t"
    "ld.shared.v2.f64 {fd73, fd74}, [r16+96];\n\t"
    "add.f64 %1, fd62, fd70;\n\t"
    "add.f64 %0, fd61, fd69;\n\t"
    "add.f64 %3, fd66, fd74;\n\t"
    "add.f64 %2, fd65, fd73;\n\t"
    "sub.f64 %5, fd62, fd70;\n\t"
    "sub.f64 %4, fd61, fd69;\n\t"
    "sub.f64 %7, fd66, fd74;\n\t"
    "sub.f64 %6, fd65, fd73;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y), "=d"(rmem[2].x), "=d"(rmem[2].y), "=d"(rmem[3].x), "=d"(rmem[3].y): "l"(smem), "l"(lut_dp_4_8), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y), "d"(rmem[1].y), "d"(rmem[2].x), "d"(rmem[2].y), "d"(rmem[2].y), "d"(rmem[3].x), "d"(rmem[3].y));
};




template<> __forceinline__ __device__ void cufftdx_private_function<445, double, 1>(cufftdx::detail::complex<double> *rmem, void *smem){

asm volatile ("{\n\t"
    ".reg .b32 r<30>;\n\t"
    ".reg .f64 fd<55>;\n\t"
    ".reg .b64 rd<8>;\n\t"
    "mov.u32 r1, %tid.x;\n\t"
    "add.f64 fd1, %7, %9;\n\t"
    "add.f64 fd2, %8, %10;\n\t"
    "sub.f64 fd17, %7, %9;\n\t"
    "sub.f64 fd18, %8, %10;\n\t"
    "and.b32 r2, r1, 3;\n\t"
    "mul.wide.u32 rd2, r2, 16;\n\t"
    "mov.u64 rd3, %5;\n\t"
    "add.s64 rd4, rd3, rd2;\n\t"
    "ld.global.v2.f64 {fd19, fd20}, [rd4];\n\t"
    "mul.f64 fd23, fd19, fd17;\n\t"
    "mul.f64 fd24, fd20, fd18;\n\t"
    "sub.f64 fd3, fd23, fd24;\n\t"
    "mul.f64 fd25, fd19, fd18;\n\t"
    "fma.rn.f64 fd4, fd20, fd17, fd25;\n\t"
    "mov.u32 r6, %tid.y;\n\t"
    "shl.b32 r7, r6, 3;\n\t"
    "shl.b32 r8, r1, 1;\n\t"
    "and.b32 r9, r8, -8;\n\t"
    "add.s32 r3, r9, r7;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r10, r2, 1;\n\t"
    "add.s32 r11, r10, r3;\n\t"
    "shl.b32 r12, r11, 4;\n\t"
    "{\n\t"
    ".reg .u64 wide1;\n\t"
    ".reg .u32 narrow1;\n\t"
    "mov.u64 wide1, %4;\n\t"
    "cvt.u32.u64 narrow1, wide1;\n\t"
    "cvta.to.shared.u32 r13, narrow1;\n\t"
    "}\n\t"
    "add.s32 r14, r13, r12;\n\t"
    "st.shared.v2.f64 [r14], {fd1, fd2};\n\t"
    "st.shared.v2.f64 [r14+16], {fd3, fd4};\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r15, r2, r3;\n\t"
    "shl.b32 r16, r15, 4;\n\t"
    "add.s32 r18, r13, r16;\n\t"
    "ld.shared.v2.f64 {fd26, fd27}, [r18];\n\t"
    "ld.shared.v2.f64 {fd30, fd31}, [r18+64];\n\t"
    "add.f64 fd5, fd26, fd30;\n\t"
    "add.f64 fd6, fd27, fd31;\n\t"
    "sub.f64 fd34, fd26, fd30;\n\t"
    "sub.f64 fd35, fd27, fd31;\n\t"
    "and.b32 r4, r1, 2;\n\t"
    "bfe.u32 r19, r1, 1, 1;\n\t"
    "mul.wide.u32 rd5, r19, 16;\n\t"
    "mov.u64 rd6, %6;\n\t"
    "add.s64 rd7, rd6, rd5;\n\t"
    "ld.global.v2.f64 {fd36, fd37}, [rd7];\n\t"
    "mul.f64 fd40, fd36, fd34;\n\t"
    "mul.f64 fd41, fd37, fd35;\n\t"
    "sub.f64 fd7, fd40, fd41;\n\t"
    "mul.f64 fd42, fd36, fd35;\n\t"
    "fma.rn.f64 fd8, fd37, fd34, fd42;\n\t"
    "and.b32 r20, r1, 1;\n\t"
    "add.s32 r5, r3, r20;\n\t"
    "barrier.sync 0;\n\t"
    "shl.b32 r21, r4, 1;\n\t"
    "add.s32 r22, r21, r5;\n\t"
    "shl.b32 r23, r22, 4;\n\t"
    "add.s32 r25, r13, r23;\n\t"
    "st.shared.v2.f64 [r25], {fd5, fd6};\n\t"
    "st.shared.v2.f64 [r25+32], {fd7, fd8};\n\t"
    "barrier.sync 0;\n\t"
    "add.s32 r26, r4, r5;\n\t"
    "shl.b32 r27, r26, 4;\n\t"
    "add.s32 r29, r13, r27;\n\t"
    "ld.shared.v2.f64 {fd43, fd44}, [r29];\n\t"
    "ld.shared.v2.f64 {fd47, fd48}, [r29+64];\n\t"
    "add.f64 %1, fd44, fd48;\n\t"
    "add.f64 %0, fd43, fd47;\n\t"
    "sub.f64 %3, fd44, fd48;\n\t"
    "sub.f64 %2, fd43, fd47;\n\t"
    "}"
     : "=d"(rmem[0].x), "=d"(rmem[0].y), "=d"(rmem[1].x), "=d"(rmem[1].y): "l"(smem), "l"(lut_dp_2_8), "l"(lut_dp_2_4), "d"(rmem[0].x), "d"(rmem[0].y), "d"(rmem[1].x), "d"(rmem[1].y));
};


#endif
